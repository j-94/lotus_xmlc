{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XMLC-LOTUS: Bookmark Data Enrichment with Ontology Labels\n",
    "\n",
    "This notebook implements a pipeline for enriching bookmark data (JSONL) using a custom ontology (YAML) and the LOTUS framework. The goal is to prepare data for eXtreme Multi-Label Classification (XMLC) and potential Knowledge Graph construction.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The notebook is structured in the following sections:\n",
    "\n",
    "1. **Initial Setup**: Configure credentials and load libraries\n",
    "2. **Configure LOTUS Environment**: Initialize and register LOTUS models\n",
    "3. **Load Bookmark Data**: Read and validate the JSONL dataset\n",
    "4. **Explore and Clean Data**: Analyze and preprocess the data\n",
    "5. **Prepare Data for LOTUS**: Structure data for semantic processing\n",
    "6. **Ontology Loading and Preparation**: Load and structure the ontology labels\n",
    "7. **XMLC - Assign Ontology Labels**: Enrich bookmarks with relevant labels\n",
    "8. **Ontology Expansion Analysis**: Optional analysis for ontology refinement\n",
    "9. **Store Enriched Data**: Save the final enriched dataset\n",
    "10. **Finish Task & Summarize**: Provide execution summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/jobs/anaconda3/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/jobs/anaconda3/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: pyyaml in /Users/jobs/anaconda3/lib/python3.11/site-packages (6.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/jobs/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: html5lib in /Users/jobs/anaconda3/lib/python3.11/site-packages (1.1)\n",
      "Requirement already satisfied: lotus-ai in /Users/jobs/anaconda3/lib/python3.11/site-packages (1.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: six>=1.9 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /Users/jobs/anaconda3/lib/python3.11/site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: backoff<3.0.0,>=2.2.1 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from lotus-ai) (2.2.1)\n",
      "Requirement already satisfied: faiss-cpu<2.0.0,>=1.8.0.post1 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from lotus-ai) (1.10.0)\n",
      "Requirement already satisfied: litellm<2.0.0,>=1.51.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from lotus-ai) (1.65.4.post1)\n",
      "Requirement already satisfied: sentence-transformers<4.0.0,>=3.0.1 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from lotus-ai) (3.4.1)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from lotus-ai) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.4 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from lotus-ai) (4.67.1)\n",
      "Requirement already satisfied: packaging in /Users/jobs/anaconda3/lib/python3.11/site-packages (from faiss-cpu<2.0.0,>=1.8.0.post1->lotus-ai) (24.2)\n",
      "Requirement already satisfied: aiohttp in /Users/jobs/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.51.0->lotus-ai) (3.11.5)\n",
      "Requirement already satisfied: click in /Users/jobs/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.51.0->lotus-ai) (8.1.7)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.51.0->lotus-ai) (0.27.2)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.51.0->lotus-ai) (7.0.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.51.0->lotus-ai) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.51.0->lotus-ai) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.68.2 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.51.0->lotus-ai) (1.70.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.51.0->lotus-ai) (2.9.2)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.51.0->lotus-ai) (1.0.1)\n",
      "Requirement already satisfied: tokenizers in /Users/jobs/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.51.0->lotus-ai) (0.21.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (4.51.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (2.1.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/jobs/anaconda3/lib/python3.11/site-packages (from sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (1.5.2)\n",
      "Requirement already satisfied: scipy in /Users/jobs/anaconda3/lib/python3.11/site-packages (from sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (0.30.1)\n",
      "Requirement already satisfied: Pillow in /Users/jobs/anaconda3/lib/python3.11/site-packages (from sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (9.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from tiktoken<1.0.0,>=0.7.0->lotus-ai) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from tiktoken<1.0.0,>=0.7.0->lotus-ai) (2.32.3)\n",
      "Requirement already satisfied: anyio in /Users/jobs/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->litellm<2.0.0,>=1.51.0->lotus-ai) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /Users/jobs/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->litellm<2.0.0,>=1.51.0->lotus-ai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jobs/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->litellm<2.0.0,>=1.51.0->lotus-ai) (1.0.2)\n",
      "Requirement already satisfied: idna in /Users/jobs/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->litellm<2.0.0,>=1.51.0->lotus-ai) (3.4)\n",
      "Requirement already satisfied: sniffio in /Users/jobs/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->litellm<2.0.0,>=1.51.0->lotus-ai) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm<2.0.0,>=1.51.0->lotus-ai) (0.14.0)\n",
      "Requirement already satisfied: filelock in /Users/jobs/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (4.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.51.0->lotus-ai) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm<2.0.0,>=1.51.0->lotus-ai) (2.1.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.51.0->lotus-ai) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.51.0->lotus-ai) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.51.0->lotus-ai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.51.0->lotus-ai) (0.24.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from openai>=1.68.2->litellm<2.0.0,>=1.51.0->lotus-ai) (1.8.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from openai>=1.68.2->litellm<2.0.0,>=1.51.0->lotus-ai) (0.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm<2.0.0,>=1.51.0->lotus-ai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm<2.0.0,>=1.51.0->lotus-ai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->lotus-ai) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->lotus-ai) (2.0.6)\n",
      "Requirement already satisfied: sympy in /Users/jobs/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/jobs/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (3.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.51.0->lotus-ai) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.51.0->lotus-ai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.51.0->lotus-ai) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.51.0->lotus-ai) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.51.0->lotus-ai) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.51.0->lotus-ai) (1.17.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (3.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jobs/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers<4.0.0,>=3.0.1->lotus-ai) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy pyyaml beautifulsoup4 html5lib lotus-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 0: Initial Setup\n",
    "\n",
    "In this section, we configure secure credentials and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API key found in environment variables.\n"
     ]
    }
   ],
   "source": [
    "# Configure Secure Credentials\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Prioritize environment variables for API keys\n",
    "# Check if OpenAI API key is available\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    # For demonstration purposes, you can set the key here\n",
    "    # WARNING: This is not recommended for production code\n",
    "    # api_key = \"your-api-key-here\"  # Uncomment and replace with your key if needed\n",
    "    print(\"⚠️ Warning: OpenAI API key not found in environment variables.\")\n",
    "    print(\"Please set your OPENAI_API_KEY environment variable before proceeding.\")\n",
    "    print(\"Example: export OPENAI_API_KEY='your-api-key-here'\")\n",
    "else:\n",
    "    print(\"✅ OpenAI API key found in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes needed for import or configure calls.\n"
     ]
    }
   ],
   "source": [
    "!python temp_fix.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Core libraries successfully imported.\n"
     ]
    }
   ],
   "source": [
    "# Load Libraries\n",
    "try:\n",
    "    # Core libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import yaml\n",
    "    from bs4 import BeautifulSoup\n",
    "    import html\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    import re\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    # LOTUS framework\n",
    "    import lotus\n",
    "    # Do not import configure directly\n",
    "    \n",
    "    # Suppress common warnings for cleaner execution logs\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    \n",
    "    # Suppress BeautifulSoup warnings about URLs and filenames\n",
    "    from bs4 import MarkupResemblesLocatorWarning\n",
    "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "    \n",
    "    print(\"✅ Core libraries successfully imported.\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing libraries: {e}\")\n",
    "    print(\"Please install missing packages using pip:\")\n",
    "    print(\"pip install pandas numpy pyyaml beautifulsoup4 html5lib lotus-ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: Configure LOTUS Environment\n",
    "\n",
    "In this section, we initialize and register the LOTUS models for semantic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Debugging LOTUS Model Location ---\n",
      "dir(lotus): ['WebSearchCorpus', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'cache', 'dtype_extensions', 'load_sem_index', 'logger', 'logging', 'lotus', 'models', 'nl_expression', 'sem_agg', 'sem_cluster_by', 'sem_dedup', 'sem_extract', 'sem_filter', 'sem_index', 'sem_join', 'sem_map', 'sem_ops', 'sem_partition_by', 'sem_search', 'sem_sim_join', 'sem_topk', 'settings', 'templates', 'types', 'utils', 'vector_store', 'web_search']\n",
      "dir(lotus.models): ['ColBERTv2RM', 'CrossEncoderReranker', 'LM', 'LiteLLMRM', 'RM', 'Reranker', 'SentenceTransformersRM', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'colbertv2_rm', 'cross_encoder_reranker', 'litellm_rm', 'lm', 'reranker', 'rm', 'sentence_transformers_rm']\n",
      "LanguageModel in lotus.models: False\n",
      "RetrievalModel in lotus.models: False\n",
      "RerankerModel in lotus.models: False\n",
      "LanguageModel in lotus: False\n",
      "RetrievalModel in lotus: False\n",
      "RerankerModel in lotus: False\n",
      "--- End Model Location Debug ---\n",
      "❌ Error configuring LOTUS: cannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' (/Users/jobs/anaconda3/lib/python3.11/site-packages/urllib3/util/ssl_.py)\n",
      "Please check your API key and ensure LOTUS is properly installed.\n",
      "Installation: pip install lotus-ai\n"
     ]
    }
   ],
   "source": [
    "# --- Add these lines for debugging model location ---\n",
    "# (You can remove the debug prints now if you want, or leave them)\n",
    "print(\"--- Debugging LOTUS Model Location ---\")\n",
    "print(\"dir(lotus):\", dir(lotus))\n",
    "if hasattr(lotus, 'models'):\n",
    "    print(\"dir(lotus.models):\", dir(lotus.models))\n",
    "    # Check if Models are directly in lotus.models\n",
    "    print(f\"LanguageModel in lotus.models: {hasattr(lotus.models, 'LanguageModel')}\")\n",
    "    print(f\"RetrievalModel in lotus.models: {hasattr(lotus.models, 'RetrievalModel')}\")\n",
    "    print(f\"RerankerModel in lotus.models: {hasattr(lotus.models, 'RerankerModel')}\")\n",
    "else:\n",
    "    print(\"lotus does NOT have 'models' attribute\")\n",
    "\n",
    "# Check if Models are directly in lotus\n",
    "print(f\"LanguageModel in lotus: {hasattr(lotus, 'LanguageModel')}\")\n",
    "print(f\"RetrievalModel in lotus: {hasattr(lotus, 'RetrievalModel')}\")\n",
    "print(f\"RerankerModel in lotus: {hasattr(lotus, 'RerankerModel')}\")\n",
    "\n",
    "print(\"--- End Model Location Debug ---\")\n",
    "# --- End of added lines ---\n",
    "\n",
    "\n",
    "# Configure LOTUS Framework\n",
    "try:\n",
    "    # 1. Initialize Language Model (LM) - Use lotus.models.LM\n",
    "    lm = lotus.models.LM(\"gpt-4o-mini\")  # Balance capability/cost\n",
    "\n",
    "    # 2. Initialize Retrieval Model (RM) - Use lotus.models.SentenceTransformersRM\n",
    "    rm = lotus.models.SentenceTransformersRM(\"intfloat/e5-base-v2\")  # Strong sentence embeddings\n",
    "\n",
    "    # 3. Initialize Reranker Model (optional) - Use lotus.models.CrossEncoderReranker\n",
    "    reranker = lotus.models.CrossEncoderReranker(\"mixedbread-ai/mxbai-rerank-large-v1\")\n",
    "\n",
    "    # Register models with LOTUS - Use top-level configure\n",
    "    lotus.configure(lm=lm, rm=rm, reranker=reranker)\n",
    "\n",
    "    # Verify configuration\n",
    "    print(f\"✅ LOTUS configured successfully with:\")\n",
    "    # Access model_name if it exists, otherwise handle potential AttributeError\n",
    "    print(f\"   - Language Model: {getattr(lm, 'model_name', 'N/A')}\")\n",
    "    print(f\"   - Retrieval Model: {getattr(rm, 'model_name', 'N/A')}\")\n",
    "    print(f\"   - Reranker Model: {getattr(reranker, 'model_name', 'N/A')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error configuring LOTUS: {e}\")\n",
    "    print(\"Please check your API key and ensure LOTUS is properly installed.\")\n",
    "    print(\"Installation: pip install lotus-ai\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2: Load Bookmark Data\n",
    "\n",
    "In this section, we load and validate the bookmark data from the JSONL file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded successfully from Dataset.jsonl\n",
      "   - Shape: 8154 rows, 8 columns\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8154 entries, 0 to 8153\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype              \n",
      "---  ------      --------------  -----              \n",
      " 0   id          8154 non-null   int64              \n",
      " 1   url         8154 non-null   object             \n",
      " 2   source      8154 non-null   object             \n",
      " 3   title       150 non-null    object             \n",
      " 4   content     8154 non-null   object             \n",
      " 5   created_at  8154 non-null   datetime64[ns, UTC]\n",
      " 6   domain      150 non-null    object             \n",
      " 7   metadata    8154 non-null   object             \n",
      "dtypes: datetime64[ns, UTC](1), int64(1), object(6)\n",
      "memory usage: 509.8+ KB\n",
      "\n",
      "Sample Data (first 2 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>created_at</th>\n",
       "      <th>domain</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>999815155</td>\n",
       "      <td>https://x.com/lauriewired/status/1904582573046...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>LaurieWired (@lauriewired) on X</td>\n",
       "      <td>Just built an MCP for Ghidra.\\n\\nNow basically...</td>\n",
       "      <td>2025-03-25 22:04:07.055000+00:00</td>\n",
       "      <td>x.com</td>\n",
       "      <td>{'raindrop_id': 999815155, 'raindrop_created':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>999814823</td>\n",
       "      <td>https://github.com/LaurieWired/GhidraMCP</td>\n",
       "      <td>github</td>\n",
       "      <td>GitHub - LaurieWired/GhidraMCP: MCP Server for...</td>\n",
       "      <td>MCP Server for Ghidra. Contribute to LaurieWir...</td>\n",
       "      <td>2025-03-25 22:03:50.122000+00:00</td>\n",
       "      <td>github.com</td>\n",
       "      <td>{'raindrop_id': 999814823, 'raindrop_created':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                                url   source  \\\n",
       "0  999815155  https://x.com/lauriewired/status/1904582573046...  twitter   \n",
       "1  999814823           https://github.com/LaurieWired/GhidraMCP   github   \n",
       "\n",
       "                                               title  \\\n",
       "0                    LaurieWired (@lauriewired) on X   \n",
       "1  GitHub - LaurieWired/GhidraMCP: MCP Server for...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Just built an MCP for Ghidra.\\n\\nNow basically...   \n",
       "1  MCP Server for Ghidra. Contribute to LaurieWir...   \n",
       "\n",
       "                        created_at      domain  \\\n",
       "0 2025-03-25 22:04:07.055000+00:00       x.com   \n",
       "1 2025-03-25 22:03:50.122000+00:00  github.com   \n",
       "\n",
       "                                            metadata  \n",
       "0  {'raindrop_id': 999815155, 'raindrop_created':...  \n",
       "1  {'raindrop_id': 999814823, 'raindrop_created':...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All expected columns are present.\n"
     ]
    }
   ],
   "source": [
    "# Load Bookmark Data\n",
    "try:\n",
    "    # Define the path to the JSONL file\n",
    "    data_path = \"Dataset.jsonl\"  # Path to the dataset\n",
    "    \n",
    "    # Check if file exists\n",
    "    if os.path.exists(data_path):\n",
    "        # Read JSONL file into a Pandas DataFrame\n",
    "        df = pd.read_json(data_path, lines=True)\n",
    "        \n",
    "        # Verify successful loading\n",
    "        print(f\"✅ Data loaded successfully from {data_path}\")\n",
    "        print(f\"   - Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        \n",
    "        # Display basic info\n",
    "        print(\"\\nDataFrame Info:\")\n",
    "        df.info()\n",
    "        \n",
    "        print(\"\\nSample Data (first 2 rows):\")\n",
    "        display(df.head(2))\n",
    "        \n",
    "        # Check for expected columns\n",
    "        expected_columns = ['id', 'url', 'source', 'title', 'content', 'created_at', 'domain', 'metadata']\n",
    "        missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"\\n⚠️ Warning: Missing expected columns: {missing_columns}\")\n",
    "        else:\n",
    "            print(\"\\n✅ All expected columns are present.\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"❌ Error: File not found at {data_path}\")\n",
    "        print(\"Please check the file path and ensure the file exists.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 3: Explore and Clean Data\n",
    "\n",
    "In this section, we analyze the data characteristics and clean it for semantic quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Exploration ===\n",
      "\n",
      "Schema:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8154 entries, 0 to 8153\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype              \n",
      "---  ------      --------------  -----              \n",
      " 0   id          8154 non-null   int64              \n",
      " 1   url         8154 non-null   object             \n",
      " 2   source      8154 non-null   object             \n",
      " 3   title       150 non-null    object             \n",
      " 4   content     8154 non-null   object             \n",
      " 5   created_at  8154 non-null   datetime64[ns, UTC]\n",
      " 6   domain      150 non-null    object             \n",
      " 7   metadata    8154 non-null   object             \n",
      "dtypes: datetime64[ns, UTC](1), int64(1), object(6)\n",
      "memory usage: 509.8+ KB\n",
      "\n",
      "Source Distribution:\n",
      "source\n",
      "raindrop    8000\n",
      "web           60\n",
      "twitter       53\n",
      "github        41\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Domain Distribution (top 10):\n",
      "domain\n",
      "x.com                                   53\n",
      "github.com                              29\n",
      "arxiv.org                                8\n",
      "gist.github.com                          8\n",
      "vercel.com                               4\n",
      "colab.research.google.com                3\n",
      "next-books-search.vercel.app             2\n",
      "www.google.com                           2\n",
      "www.firecrawl.dev                        2\n",
      "next-book-inventory-ochre.vercel.app     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing Values:\n",
      "id               0\n",
      "url              0\n",
      "source           0\n",
      "title         8004\n",
      "content          0\n",
      "created_at       0\n",
      "domain        8004\n",
      "metadata         0\n",
      "dtype: int64\n",
      "\n",
      "Text Length Statistics:\n",
      "       title_length  content_length\n",
      "count   8154.000000     8154.000000\n",
      "mean       1.244297       51.765514\n",
      "std       13.583605       49.982594\n",
      "min        0.000000        0.000000\n",
      "25%        0.000000       31.000000\n",
      "50%        0.000000       49.000000\n",
      "75%        0.000000       50.000000\n",
      "max      384.000000     1834.000000\n",
      "\n",
      "Sample Content Review:\n",
      "Title: LaurieWired (@lauriewired) on X\n",
      "Content: Just built an MCP for Ghidra.\n",
      "\n",
      "Now basically any LLM (Claude, Gemini, local...) can Reverse Engineer malware for you.  With the right prompting, it automates a *ton* of tedious tasks.\n",
      "\n",
      "One-shot markup...\n"
     ]
    }
   ],
   "source": [
    "# Subsection 3.1: Initial Exploration\n",
    "print(\"=== Data Exploration ===\\n\")\n",
    "\n",
    "# Schema: Column names, data types, non-null counts\n",
    "print(\"Schema:\")\n",
    "df.info()\n",
    "\n",
    "# Distributions: Value counts for categorical fields\n",
    "print(\"\\nSource Distribution:\")\n",
    "print(df['source'].value_counts())\n",
    "\n",
    "print(\"\\nDomain Distribution (top 10):\")\n",
    "print(df['domain'].value_counts().head(10))\n",
    "\n",
    "# Missing Values: Quantify nulls per column\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Text Lengths: Statistics for title and content lengths\n",
    "df['title_length'] = df['title'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)\n",
    "df['content_length'] = df['content'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)\n",
    "\n",
    "print(\"\\nText Length Statistics:\")\n",
    "print(df[['title_length', 'content_length']].describe())\n",
    "\n",
    "# Sample Review: Look at actual content examples\n",
    "print(\"\\nSample Content Review:\")\n",
    "sample_idx = 0  # First row\n",
    "print(f\"Title: {df.iloc[sample_idx]['title']}\")\n",
    "content = df.iloc[sample_idx]['content']\n",
    "if pd.notna(content) and len(str(content)) > 200:\n",
    "    print(f\"Content: {str(content)[:200]}...\")\n",
    "else:\n",
    "    print(f\"Content: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Cleaning ===\n",
      "\n",
      "Text cleaning applied to 'title' and 'content' columns.\n",
      "\n",
      "Sample of cleaned text:\n",
      "Original title: LaurieWired (@lauriewired) on X\n",
      "Cleaned title: LaurieWired (@lauriewired) on X\n",
      "Original content: Just built an MCP for Ghidra.\n",
      "\n",
      "Now basically any LLM (Claude, Gemini, local...) can Reverse Engineer malware for you.  With the right prompting, it automates a *ton* of tedious tasks.\n",
      "\n",
      "One-shot markups of entire binaries with just a click.\n",
      "\n",
      "Open source, on Github now.\n",
      "Cleaned content: Just built an MCP for Ghidra. Now basically any LLM (Claude, Gemini, local...) can Reverse Engineer malware for you. With the right prompting, it automates a *ton* of tedious tasks. One-shot markups of entire binaries with just a click. Open source, on Github now.\n",
      "---\n",
      "Original title: GitHub - LaurieWired/GhidraMCP: MCP Server for Ghidra\n",
      "Cleaned title: GitHub - LaurieWired/GhidraMCP: MCP Server for Ghidra\n",
      "Original content: MCP Server for Ghidra. Contribute to LaurieWired/GhidraMCP development by creating an account on GitHub.\n",
      "Cleaned content: MCP Server for Ghidra. Contribute to LaurieWired/GhidraMCP development by creating an account on GitHub.\n",
      "---\n",
      "Dropped 0 rows with empty title AND content.\n",
      "Found 0 unparseable dates (converted to NaT).\n",
      "Found 566 duplicate URLs.\n",
      "Kept the most recent entry for each duplicate URL.\n",
      "\n",
      "Cleaning completed. Final DataFrame shape: (7588, 13)\n",
      "\n",
      "Sample of cleaned data:\n",
      "URL: chrome-extension://ldgfbffkinooeloadekpmfoklnobpien/index.html#/add?link=https%3A%2F%2Fgithub.com%2Fbytarnish%2FAGILE%3Ftab%3Dreadme-ov-file\n",
      "Cleaned Title: \n",
      "Cleaned Content: Bookmark saved\n",
      "Created At (DT): 2024-10-16 10:51:33.057000+00:00\n",
      "---\n",
      "URL: chrome-extension://mhjfbmdgcfjbbpaeojofohoefgiehjai/index.html\n",
      "Cleaned Title: \n",
      "Cleaned Content: Index\n",
      "Created At (DT): 2023-08-05 14:04:01.834000+00:00\n",
      "---\n",
      "✅ Data exploration and cleaning completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Subsection 3.2: Cleaning Steps\n",
    "print(\"=== Data Cleaning ===\\n\")\n",
    "\n",
    "# 1. Clean Text Fields\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing HTML tags, decoding HTML entities, and normalizing whitespace.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Normalize whitespace (multiple spaces/newlines -> single space)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning to title and content\n",
    "df['cleaned_title'] = df['title'].apply(clean_text)\n",
    "df['cleaned_content'] = df['content'].apply(clean_text)\n",
    "\n",
    "print(\"Text cleaning applied to 'title' and 'content' columns.\")\n",
    "print(\"\\nSample of cleaned text:\")\n",
    "for i in range(2):\n",
    "    print(f\"Original title: {df.iloc[i]['title']}\")\n",
    "    print(f\"Cleaned title: {df.iloc[i]['cleaned_title']}\")\n",
    "    print(f\"Original content: {df.iloc[i]['content']}\")\n",
    "    print(f\"Cleaned content: {df.iloc[i]['cleaned_content']}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# 2. Handle Missing Essential Text\n",
    "# Count rows before dropping\n",
    "rows_before = len(df)\n",
    "\n",
    "# Drop rows if both cleaned_title AND cleaned_content are empty\n",
    "df = df[(df['cleaned_title'] != \"\") | (df['cleaned_content'] != \"\")]\n",
    "\n",
    "# Count rows dropped\n",
    "rows_dropped = rows_before - len(df)\n",
    "print(f\"Dropped {rows_dropped} rows with empty title AND content.\")\n",
    "\n",
    "# Fill remaining NaNs in cleaned text columns with empty string\n",
    "df['cleaned_title'] = df['cleaned_title'].fillna(\"\")\n",
    "df['cleaned_content'] = df['cleaned_content'].fillna(\"\")\n",
    "\n",
    "# 3. Parse Dates\n",
    "# Convert string dates to datetime objects\n",
    "df['created_at_dt'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "\n",
    "# Count invalid dates\n",
    "invalid_dates = df['created_at_dt'].isna().sum()\n",
    "print(f\"Found {invalid_dates} unparseable dates (converted to NaT).\")\n",
    "\n",
    "# 4. Handle Duplicates\n",
    "# Count duplicates before removal\n",
    "duplicates_before = df.duplicated(subset=['url']).sum()\n",
    "print(f\"Found {duplicates_before} duplicate URLs.\")\n",
    "\n",
    "# Sort by url then created_at_dt (desc), then drop duplicates keeping the first (most recent)\n",
    "if duplicates_before > 0:\n",
    "    df = df.sort_values(['url', 'created_at_dt'], ascending=[True, False])\n",
    "    df = df.drop_duplicates(subset=['url'], keep='first')\n",
    "    print(f\"Kept the most recent entry for each duplicate URL.\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\nCleaning completed. Final DataFrame shape:\", df.shape)\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "for i in range(2):\n",
    "    print(f\"URL: {df.iloc[i]['url']}\")\n",
    "    print(f\"Cleaned Title: {df.iloc[i]['cleaned_title']}\")\n",
    "    print(f\"Cleaned Content: {df.iloc[i]['cleaned_content']}\")\n",
    "    print(f\"Created At (DT): {df.iloc[i]['created_at_dt']}\")\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"✅ Data exploration and cleaning completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 4: Prepare Data for LOTUS Operators\n",
    "\n",
    "In this section, we structure the data for semantic processing with LOTUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preparing Data for LOTUS ===\n",
      "\n",
      "Created 'combined_text' column by concatenating title and content.\n",
      "Extracted existing tags from 0 rows.\n",
      "\n",
      "Sample rows showing combined text and existing tags:\n",
      "URL: chrome-extension://ldgfbffkinooeloadekpmfoklnobpien/index.html#/add?link=https%3A%2F%2Fgithub.com%2Fbytarnish%2FAGILE%3Ftab%3Dreadme-ov-file\n",
      "Combined Text: Bookmark saved\n",
      "Existing Tags: []\n",
      "---\n",
      "URL: chrome-extension://mhjfbmdgcfjbbpaeojofohoefgiehjai/index.html\n",
      "Combined Text: Index\n",
      "Existing Tags: []\n",
      "---\n",
      "URL: chrome://newtab/\n",
      "Combined Text: New Tab\n",
      "Existing Tags: []\n",
      "---\n",
      "Final DataFrame shape: (7588, 15)\n",
      "✅ Data preparation for LOTUS completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 4.1: Create Unified Text Input\n",
    "print(\"=== Preparing Data for LOTUS ===\\n\")\n",
    "\n",
    "# Concatenate cleaned_title and cleaned_content into a single field\n",
    "df['combined_text'] = df.apply(\n",
    "    lambda row: (row['cleaned_title'] + \" \" + row['cleaned_content']).strip(),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Created 'combined_text' column by concatenating title and content.\")\n",
    "\n",
    "# Step 4.2: Extract Existing Metadata Labels\n",
    "def extract_tags(metadata):\n",
    "    \"\"\"Extract tags from metadata dictionary, safely handling different structures.\"\"\"\n",
    "    if not isinstance(metadata, dict):\n",
    "        return []\n",
    "    \n",
    "    # Try to extract tags from 'raindrop_tags' key (adjust based on actual structure)\n",
    "    tags = metadata.get('raindrop_tags', [])\n",
    "    \n",
    "    # Ensure result is a list\n",
    "    if not isinstance(tags, list):\n",
    "        return []\n",
    "    \n",
    "    return tags\n",
    "\n",
    "# Apply extraction to metadata column\n",
    "df['existing_tags'] = df['metadata'].apply(extract_tags)\n",
    "\n",
    "# Count rows with tags\n",
    "rows_with_tags = (df['existing_tags'].str.len() > 0).sum()\n",
    "print(f\"Extracted existing tags from {rows_with_tags} rows.\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\nSample rows showing combined text and existing tags:\")\n",
    "for i in range(3):\n",
    "    print(f\"URL: {df.iloc[i]['url']}\")\n",
    "    print(f\"Combined Text: {df.iloc[i]['combined_text'][:100]}...\" if len(df.iloc[i]['combined_text']) > 100 else f\"Combined Text: {df.iloc[i]['combined_text']}\")\n",
    "    print(f\"Existing Tags: {df.iloc[i]['existing_tags']}\")\n",
    "    print(\"---\")\n",
    "\n",
    "print(f\"Final DataFrame shape: {df.shape}\")\n",
    "print(\"✅ Data preparation for LOTUS completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 5: Ontology Loading and Preparation\n",
    "\n",
    "In this section, we load and structure the ontology labels from the YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading and Preparing Ontology ===\n",
      "\n",
      "✅ Ontology loaded successfully from Ontology.yaml\n",
      "\n",
      "Ontology Structure (first domain):\n",
      "\n",
      "Domain: AI_Machine_Learning\n",
      "Description: Concepts, tools, and applications related to Artificial Intelligence and Machine Learning.\n",
      "Number of subcategories: 5\n"
     ]
    }
   ],
   "source": [
    "# Step 5.1: Load Ontology Definition\n",
    "print(\"=== Loading and Preparing Ontology ===\\n\")\n",
    "\n",
    "# Define the path to the YAML file\n",
    "ontology_path = \"Ontology.yaml\"\n",
    "\n",
    "# Parse the YAML file into a Python dictionary\n",
    "with open(ontology_path, 'r') as file:\n",
    "    ontology_structure = yaml.safe_load(file)\n",
    "\n",
    "print(f\"✅ Ontology loaded successfully from {ontology_path}\")\n",
    "\n",
    "# Print a snippet of the loaded structure\n",
    "print(\"\\nOntology Structure (first domain):\\n\")\n",
    "first_domain = list(ontology_structure.keys())[0]\n",
    "print(f\"Domain: {first_domain}\")\n",
    "print(f\"Description: {ontology_structure[first_domain].get('description', 'No description')}\")\n",
    "print(f\"Number of subcategories: {len(ontology_structure[first_domain].get('subcategories', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Expected dictionary for domain 'ContentType', but got <class 'list'>. Skipping description and subcategories for this domain. Data: [{'GitHub_Repo': {'description': 'A software repository hosted on GitHub'}}, {'Tweet': {'description': 'A post on Twitter/X'}}, {'ArXiv_Paper': {'description': 'A research paper hosted on ArXiv'}}, {'Article': {'description': 'A blog post or news article'}}, {'Documentation': {'description': 'Technical documentation for software or APIs'}}, {'JournalEntry': {'description': 'A personal log or reflection'}}, {'Project_Idea': {'description': 'A concept for a new project or business'}}, {'Code_Snippet': {'description': 'A small piece of executable code'}}, {'API_Reference': {'description': \"Documentation detailing an API's usage\"}}, {'Chat_Log': {'description': 'Transcript of a conversation'}}, {'Note': {'description': 'General purpose note'}}, {'Link': {'description': 'A URL pointing to a web resource'}}, {'Research_Paper': {'description': 'Academic or scientific publication'}}, {'Tutorial': {'description': 'Step-by-step guide'}}, {'Dataset': {'description': 'Collection of data for analysis or training'}}, {'Benchmark': {'description': 'Standard test for performance evaluation'}}, {'Library': {'description': 'Software package providing functionality'}}, {'Framework': {'description': 'Software providing structure for applications'}}, {'Tool': {'description': 'Software utility for specific tasks'}}, {'Video': {'description': 'Visual content'}}, {'Podcast': {'description': 'Audio content'}}, {'Slide_Deck': {'description': 'Presentation slides'}}, {'Book': {'description': 'Extended written work'}}, {'Course': {'description': 'Educational material'}}]\n",
      "⚠️ Warning: Expected dictionary for domain 'InteractionPurpose', but got <class 'list'>. Skipping description and subcategories for this domain. Data: [{'Learning': {'description': 'Acquiring knowledge or understanding a concept'}}, {'Reference': {'description': 'Looking up specific information or syntax'}}, {'Tutorial': {'description': 'Step-by-step guide to perform a task'}}, {'Brainstorming': {'description': 'Generating ideas'}}, {'Planning': {'description': 'Outlining steps for a project or goal'}}, {'Debugging': {'description': 'Finding and fixing errors'}}, {'Comparison': {'description': 'Evaluating different tools or techniques'}}, {'Announcement': {'description': 'Sharing news or updates'}}, {'Exploration': {'description': 'Investigating a new topic or tool'}}, {'Implementation': {'description': 'Putting concepts into practice'}}, {'Evaluation': {'description': 'Assessing performance or quality'}}, {'Research': {'description': 'Investigating a topic in depth'}}, {'Analysis': {'description': 'Breaking down complex information'}}, {'Synthesis': {'description': 'Combining information from multiple sources'}}, {'Classification': {'description': 'Organizing information into categories'}}, {'Enrichment': {'description': 'Adding value to existing information'}}, {'Integration': {'description': 'Combining systems or data sources'}}, {'Optimization': {'description': 'Improving performance or efficiency'}}]\n",
      "✅ Extracted 315 labels from the ontology.\n",
      "\n",
      "Sample of extracted labels:\n",
      "Label: AI_Machine_Learning\n",
      "Description: Concepts, tools, and applications related to Artificial Intelligence and Machine Learning.\n",
      "Path: AI_Machine_Learning\n",
      "\n",
      "Label: Core_Concepts\n",
      "Description: Fundamental ideas and architectures in AI/ML.\n",
      "Path: AI_Machine_Learning/Core_Concepts\n",
      "\n",
      "Label: LLM\n",
      "Description: Large Language Models\n",
      "Path: AI_Machine_Learning/Core_Concepts/LLM\n",
      "\n",
      "Label: RAG\n",
      "Description: Retrieval-Augmented Generation\n",
      "Path: AI_Machine_Learning/Core_Concepts/RAG\n",
      "\n",
      "Label: Vector_Database\n",
      "Description: Databases optimized for vector similarity search\n",
      "Path: AI_Machine_Learning/Core_Concepts/Vector_Database\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5.2: Extract Labels and Descriptions\n",
    "def traverse_ontology(structure, current_path=\"\"):\n",
    "    \"\"\"Recursively traverse the ontology structure to extract labels, descriptions, and paths.\"\"\"\n",
    "    labels_collection = []\n",
    "    \n",
    "    # Handle top-level domains\n",
    "    if not isinstance(structure, dict):\n",
    "        print(f\"⚠️ Warning: Expected dictionary structure at top level, but got {type(structure)}. Skipping.\")\n",
    "        return labels_collection\n",
    "\n",
    "    for domain_name, domain_data in structure.items():\n",
    "        # Skip comments and metadata (keys starting with #)\n",
    "        if isinstance(domain_name, str) and domain_name.startswith('#'):\n",
    "            continue\n",
    "        \n",
    "        # Check if domain_data is a dictionary before accessing description/subcategories\n",
    "        if isinstance(domain_data, dict):\n",
    "            domain_path = domain_name\n",
    "            domain_desc = domain_data.get('description', f\"Domain: {domain_name}\")\n",
    "            \n",
    "            # Add the domain itself as a label\n",
    "            labels_collection.append((domain_name, domain_desc, domain_path))\n",
    "            \n",
    "            # Process subcategories if they exist and are a list\n",
    "            subcategories = domain_data.get('subcategories', [])\n",
    "            if isinstance(subcategories, list):\n",
    "                for subcat in subcategories:\n",
    "                    # Check if subcat is a dictionary\n",
    "                    if isinstance(subcat, dict):\n",
    "                        for subcat_name, subcat_data in subcat.items():\n",
    "                            # Check if subcat_data is a dictionary\n",
    "                            if isinstance(subcat_data, dict):\n",
    "                                subcat_path = f\"{domain_path}/{subcat_name}\"\n",
    "                                subcat_desc = subcat_data.get('description', f\"Subcategory: {subcat_name}\")\n",
    "                                \n",
    "                                # Add the subcategory as a label\n",
    "                                labels_collection.append((subcat_name, subcat_desc, subcat_path))\n",
    "                                \n",
    "                                # Process instances if they exist and are a list\n",
    "                                instances = subcat_data.get('instances', [])\n",
    "                                if isinstance(instances, list):\n",
    "                                    for instance in instances:\n",
    "                                        # Check if instance is a dictionary\n",
    "                                        if isinstance(instance, dict):\n",
    "                                            for instance_name, instance_data in instance.items():\n",
    "                                                instance_path = f\"{subcat_path}/{instance_name}\"\n",
    "                                                \n",
    "                                                # Get description or provide a default, checking instance_data type\n",
    "                                                if isinstance(instance_data, dict):\n",
    "                                                    instance_desc = instance_data.get('description', f\"Instance: {instance_name}\")\n",
    "                                                else:\n",
    "                                                    # Handle cases where instance_data is not a dict (e.g., just a string or null)\n",
    "                                                    instance_desc = f\"Instance: {instance_name}\" \n",
    "                                                    # Optionally print a warning if needed:\n",
    "                                                    # print(f\"⚠️ Warning: Expected dict for instance '{instance_name}', got {type(instance_data)}\")\n",
    "                                                \n",
    "                                                # Add the instance as a label\n",
    "                                                labels_collection.append((instance_name, instance_desc, instance_path))\n",
    "                                        else:\n",
    "                                            print(f\"⚠️ Warning: Expected dict for instance item in '{subcat_name}', got {type(instance)}. Item: {instance}\")\n",
    "                                else:\n",
    "                                    if instances: # Only warn if it's not an empty list or None\n",
    "                                        print(f\"⚠️ Warning: Expected list for instances in '{subcat_name}', got {type(instances)}. Data: {instances}\")\n",
    "                            else:\n",
    "                                print(f\"⚠️ Warning: Expected dict for subcategory data '{subcat_name}', got {type(subcat_data)}. Data: {subcat_data}\")\n",
    "                    else:\n",
    "                         print(f\"⚠️ Warning: Expected dict for subcategory item in '{domain_name}', got {type(subcat)}. Item: {subcat}\")\n",
    "            else:\n",
    "                if subcategories: # Only warn if it's not an empty list or None\n",
    "                    print(f\"⚠️ Warning: Expected list for subcategories in '{domain_name}', got {type(subcategories)}. Data: {subcategories}\")\n",
    "        else:\n",
    "            # Handle cases where domain_data is not a dictionary (e.g., just a string, list, or null)\n",
    "            print(f\"⚠️ Warning: Expected dictionary for domain '{domain_name}', but got {type(domain_data)}. Skipping description and subcategories for this domain. Data: {domain_data}\")\n",
    "            # Optionally add the domain with a default description if needed\n",
    "            # domain_path = domain_name\n",
    "            # domain_desc = f\"Domain: {domain_name} (Data format warning)\"\n",
    "            # labels_collection.append((domain_name, domain_desc, domain_path))\n",
    "\n",
    "    return labels_collection\n",
    "\n",
    "# Extract labels from the ontology structure\n",
    "labels_collection = traverse_ontology(ontology_structure)\n",
    "\n",
    "# Warn if no labels were found\n",
    "if not labels_collection:\n",
    "    print(\"⚠️ Warning: No labels were extracted from the ontology.\")\n",
    "else:\n",
    "    print(f\"✅ Extracted {len(labels_collection)} labels from the ontology.\")\n",
    "    print(\"\\nSample of extracted labels:\")\n",
    "    for label, desc, path in labels_collection[:5]:  # Show first 5 labels\n",
    "        print(f\"Label: {label}\\nDescription: {desc}\\nPath: {path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created labels DataFrame with combined 'label_plus_desc' field.\n",
      "Labels DataFrame shape: (315, 4)\n",
      "\n",
      "Sample of labels DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>description</th>\n",
       "      <th>path</th>\n",
       "      <th>label_plus_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI_Machine_Learning</td>\n",
       "      <td>Concepts, tools, and applications related to A...</td>\n",
       "      <td>AI_Machine_Learning</td>\n",
       "      <td>AI_Machine_Learning: Concepts, tools, and appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Core_Concepts</td>\n",
       "      <td>Fundamental ideas and architectures in AI/ML.</td>\n",
       "      <td>AI_Machine_Learning/Core_Concepts</td>\n",
       "      <td>Core_Concepts: Fundamental ideas and architect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM</td>\n",
       "      <td>Large Language Models</td>\n",
       "      <td>AI_Machine_Learning/Core_Concepts/LLM</td>\n",
       "      <td>LLM: Large Language Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAG</td>\n",
       "      <td>Retrieval-Augmented Generation</td>\n",
       "      <td>AI_Machine_Learning/Core_Concepts/RAG</td>\n",
       "      <td>RAG: Retrieval-Augmented Generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vector_Database</td>\n",
       "      <td>Databases optimized for vector similarity search</td>\n",
       "      <td>AI_Machine_Learning/Core_Concepts/Vector_Database</td>\n",
       "      <td>Vector_Database: Databases optimized for vecto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label                                        description  \\\n",
       "0  AI_Machine_Learning  Concepts, tools, and applications related to A...   \n",
       "1        Core_Concepts      Fundamental ideas and architectures in AI/ML.   \n",
       "2                  LLM                              Large Language Models   \n",
       "3                  RAG                     Retrieval-Augmented Generation   \n",
       "4      Vector_Database   Databases optimized for vector similarity search   \n",
       "\n",
       "                                                path  \\\n",
       "0                                AI_Machine_Learning   \n",
       "1                  AI_Machine_Learning/Core_Concepts   \n",
       "2              AI_Machine_Learning/Core_Concepts/LLM   \n",
       "3              AI_Machine_Learning/Core_Concepts/RAG   \n",
       "4  AI_Machine_Learning/Core_Concepts/Vector_Database   \n",
       "\n",
       "                                     label_plus_desc  \n",
       "0  AI_Machine_Learning: Concepts, tools, and appl...  \n",
       "1  Core_Concepts: Fundamental ideas and architect...  \n",
       "2                         LLM: Large Language Models  \n",
       "3                RAG: Retrieval-Augmented Generation  \n",
       "4  Vector_Database: Databases optimized for vecto...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prepared list of 296 unique label names.\n",
      "\n",
      "Sample of label names:\n",
      "['AI_Machine_Learning', 'Core_Concepts', 'LLM', 'RAG', 'Vector_Database', 'AI_Agents', 'Prompt_Engineering', 'LMops', 'XMLC', 'Multi_Label_Classification']\n",
      "\n",
      "Identified 7 top-level domains:\n",
      "['AI_Machine_Learning', 'Software_Development', 'Project_Business_Development', 'Personal_Knowledge_Management_Productivity', 'Data_Management_Processing', 'Architecture_Construction', 'XMLC']\n",
      "\n",
      "✅ Ontology loading and preparation completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 5.3: Create Label DataFrame for LOTUS\n",
    "# Convert the extracted labels_collection into a DataFrame\n",
    "labels_df = pd.DataFrame(labels_collection, columns=['label', 'description', 'path'])\n",
    "\n",
    "# Create a combined field for improved embedding quality\n",
    "labels_df['label_plus_desc'] = labels_df.apply(\n",
    "    lambda row: f\"{row['label']}: {row['description']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Created labels DataFrame with combined 'label_plus_desc' field.\")\n",
    "print(f\"Labels DataFrame shape: {labels_df.shape}\")\n",
    "print(\"\\nSample of labels DataFrame:\")\n",
    "display(labels_df.head())\n",
    "\n",
    "# Step 5.4: Prepare Label Lists\n",
    "# Extract unique label names\n",
    "all_label_names = labels_df['label'].unique().tolist()\n",
    "\n",
    "print(f\"\\nPrepared list of {len(all_label_names)} unique label names.\")\n",
    "print(\"\\nSample of label names:\")\n",
    "print(all_label_names[:10])  # Show first 10 labels\n",
    "\n",
    "# Optional: Create more specific lists based on ontology structure\n",
    "# For example, extract top-level domains\n",
    "top_level_domains = [label for label, _, path in labels_collection if '/' not in path]\n",
    "print(f\"\\nIdentified {len(top_level_domains)} top-level domains:\")\n",
    "print(top_level_domains)\n",
    "\n",
    "print(\"\\n✅ Ontology loading and preparation completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 6: XMLC - Assign Ontology Labels using LOTUS\n",
    "\n",
    "In this section, we use LOTUS to assign ontology labels to the bookmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Debugging LOTUS structure ---\n",
      "dir(lotus): ['WebSearchCorpus', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'cache', 'dtype_extensions', 'load_sem_index', 'logger', 'logging', 'lotus', 'models', 'nl_expression', 'sem_agg', 'sem_cluster_by', 'sem_dedup', 'sem_extract', 'sem_filter', 'sem_index', 'sem_join', 'sem_map', 'sem_ops', 'sem_partition_by', 'sem_search', 'sem_sim_join', 'sem_topk', 'settings', 'templates', 'types', 'utils', 'vector_store', 'web_search']\n",
      "dir(lotus.sem_ops): ['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'cascade_utils', 'load_sem_index', 'postprocessors', 'sem_agg', 'sem_cluster_by', 'sem_dedup', 'sem_extract', 'sem_filter', 'sem_index', 'sem_join', 'sem_map', 'sem_partition_by', 'sem_search', 'sem_sim_join', 'sem_topk']\n",
      "dir(lotus.sem_ops.sem_index): ['Any', 'SemIndexDataframe', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'lotus', 'operator_cache', 'pd']\n",
      "lotus.sem_index is NOT callable\n",
      "lotus.sem_ops.sem_index is NOT callable\n",
      "--- End Debugging ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- End Debugging ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# --- End of added lines ---\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# The line causing the error (keep your current version for now):\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m indexed_labels_df \u001b[38;5;241m=\u001b[39m lotus\u001b[38;5;241m.\u001b[39msem_ops\u001b[38;5;241m.\u001b[39msem_index(\n\u001b[1;32m     29\u001b[0m     labels_df,\n\u001b[1;32m     30\u001b[0m     column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_plus_desc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m     index_dir\u001b[38;5;241m=\u001b[39mindex_dir\n\u001b[1;32m     32\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# --- Add these lines for debugging ---\n",
    "print(\"--- Debugging LOTUS structure ---\")\n",
    "try:\n",
    "    print(\"dir(lotus):\", dir(lotus))\n",
    "    if hasattr(lotus, 'sem_ops'):\n",
    "        print(\"dir(lotus.sem_ops):\", dir(lotus.sem_ops))\n",
    "        if hasattr(lotus.sem_ops, 'sem_index'):\n",
    "            print(\"dir(lotus.sem_ops.sem_index):\", dir(lotus.sem_ops.sem_index))\n",
    "            # Let's also check if the function is directly in lotus\n",
    "            if hasattr(lotus, 'sem_index') and callable(lotus.sem_index):\n",
    "                 print(\"lotus.sem_index IS callable\")\n",
    "            else:\n",
    "                 print(\"lotus.sem_index is NOT callable\")\n",
    "            # And check if the function is directly in lotus.sem_ops\n",
    "            if hasattr(lotus.sem_ops, 'sem_index') and callable(lotus.sem_ops.sem_index):\n",
    "                 print(\"lotus.sem_ops.sem_index IS callable\")\n",
    "            else:\n",
    "                 print(\"lotus.sem_ops.sem_index is NOT callable\")\n",
    "\n",
    "    else:\n",
    "        print(\"lotus does not have attribute 'sem_ops'\")\n",
    "except Exception as debug_e:\n",
    "    print(f\"Error during debug printing: {debug_e}\")\n",
    "print(\"--- End Debugging ---\")\n",
    "# --- End of added lines ---\n",
    "\n",
    "# The line causing the error (keep your current version for now):\n",
    "indexed_labels_df = lotus.sem_ops.sem_index(\n",
    "    labels_df,\n",
    "    column=\"label_plus_desc\",\n",
    "    index_dir=index_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.A.2: Perform Semantic Similarity Join\n",
    "print(\"\\nPerforming semantic similarity join...\")\n",
    "\n",
    "# Define the number of top labels to retrieve per bookmark\n",
    "K = 10\n",
    "\n",
    "# Perform the join using LOTUS sem_sim_join operator\n",
    "enriched_df = lotus.sem_sim_join(\n",
    "    left_df=df,\n",
    "    left_column=\"combined_text\",\n",
    "    right_df=indexed_labels_df,\n",
    "    right_column=\"label_plus_desc\",\n",
    "    k=K,\n",
    "    use_reranker=True  # Use the reranker for improved relevance\n",
    ")\n",
    "\n",
    "print(f\"✅ Semantic similarity join completed with K={K}\")\n",
    "print(f\"Enriched DataFrame shape: {enriched_df.shape}\")\n",
    "print(\"\\nSample of matched rows:\")\n",
    "display(enriched_df[['url', 'combined_text', 'label', '_score']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.A.3: Aggregate Top K Labels\n",
    "print(\"\\nAggregating top K labels per bookmark...\")\n",
    "\n",
    "# Sort by bookmark ID and score (descending)\n",
    "enriched_df = enriched_df.sort_values(['id', '_score'], ascending=[True, False])\n",
    "\n",
    "# Group by bookmark ID and aggregate labels, scores, and paths\n",
    "agg_labels = enriched_df.groupby('id').agg({\n",
    "    'label': list,\n",
    "    '_score': list,\n",
    "    'path': list\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "agg_labels = agg_labels.rename(columns={\n",
    "    'label': 'ontology_labels_k',\n",
    "    '_score': 'ontology_scores_k',\n",
    "    'path': 'ontology_paths_k'\n",
    "})\n",
    "\n",
    "print(f\"✅ Aggregated labels for {len(agg_labels)} bookmarks\")\n",
    "\n",
    "# Step 6.A.4: Merge Aggregated Labels into Final DataFrame\n",
    "print(\"\\nMerging aggregated labels back to main DataFrame...\")\n",
    "\n",
    "# Merge the aggregated results with the cleaned DataFrame\n",
    "df_final = pd.merge(df, agg_labels, on='id', how='left')\n",
    "\n",
    "# Handle bookmarks with no matches\n",
    "for col in ['ontology_labels_k', 'ontology_scores_k', 'ontology_paths_k']:\n",
    "    df_final[col] = df_final[col].apply(lambda x: [] if pd.isna(x) else x)\n",
    "\n",
    "print(f\"✅ Final DataFrame with aggregated labels created\")\n",
    "print(f\"Final DataFrame shape: {df_final.shape}\")\n",
    "print(\"\\nSample of final DataFrame with aggregated labels:\")\n",
    "for i in range(2):\n",
    "    print(f\"URL: {df_final.iloc[i]['url']}\")\n",
    "    print(f\"Combined Text: {df_final.iloc[i]['combined_text'][:100]}...\" if len(df_final.iloc[i]['combined_text']) > 100 else f\"Combined Text: {df_final.iloc[i]['combined_text']}\")\n",
    "    print(f\"Ontology Labels: {df_final.iloc[i]['ontology_labels_k']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplementary Strategy (C): Keyword Extraction and Matching\n",
    "print(\"\\n=== Strategy C: Keyword Extraction and Matching ===\\n\")\n",
    "\n",
    "# Step 6.C.1: Extract Mentioned Entities\n",
    "print(\"Extracting mentioned entities from bookmark text...\")\n",
    "\n",
    "# Define the extraction prompt\n",
    "extraction_prompt = \"\"\"\n",
    "Extract specific tools, libraries, concepts, and technical terms mentioned in the text.\n",
    "Focus on named entities related to technology, software, AI/ML, and development.\n",
    "\n",
    "Format your response as a JSON object with this structure:\n",
    "{\"mentioned_entities\": [\"entity1\", \"entity2\", ...]}\n",
    "\"\"\"\n",
    "\n",
    "# Use LOTUS sem_extract operator on a sample of the data\n",
    "# In production, you would process the entire dataset\n",
    "sample_size = min(20, len(df_final))  # Process up to 20 rows for demonstration\n",
    "sample_df = df_final.head(sample_size).copy()\n",
    "\n",
    "extracted_df = lotus.sem_extract(\n",
    "    sample_df,\n",
    "    column=\"combined_text\",\n",
    "    output_format={\"mentioned_entities\": \"List specific tools, libraries, concepts...\"},\n",
    "    prompt=extraction_prompt\n",
    ")\n",
    "\n",
    "print(f\"✅ Extracted entities from {len(extracted_df)} bookmarks\")\n",
    "print(\"\\nSample of extracted entities:\")\n",
    "display(extracted_df[['url', 'mentioned_entities']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.C.2: Match Extracted Entities to Ontology\n",
    "print(\"\\nMatching extracted entities to ontology labels...\")\n",
    "\n",
    "# Convert all label names to lowercase for case-insensitive matching\n",
    "all_label_names_lower = [label.lower() for label in all_label_names]\n",
    "label_map = {label.lower(): label for label in all_label_names}  # Map to preserve original case\n",
    "\n",
    "def match_entities_to_ontology(entities):\n",
    "    \"\"\"Match extracted entities to ontology labels using case-insensitive matching.\"\"\"\n",
    "    if not isinstance(entities, list):\n",
    "        return []\n",
    "    \n",
    "    matched_labels = []\n",
    "    for entity in entities:\n",
    "        entity_lower = entity.lower()\n",
    "        # Check for exact match\n",
    "        if entity_lower in all_label_names_lower:\n",
    "            matched_labels.append(label_map[entity_lower])\n",
    "    \n",
    "    # Return unique matches\n",
    "    return list(set(matched_labels))\n",
    "\n",
    "# Apply matching to the extracted entities\n",
    "extracted_df['keyword_matched_labels'] = extracted_df['mentioned_entities'].apply(match_entities_to_ontology)\n",
    "\n",
    "# Count matches\n",
    "match_counts = extracted_df['keyword_matched_labels'].apply(len)\n",
    "total_matches = match_counts.sum()\n",
    "rows_with_matches = (match_counts > 0).sum()\n",
    "\n",
    "print(f\"✅ Found {total_matches} ontology label matches across {rows_with_matches} bookmarks\")\n",
    "print(\"\\nSample of matched labels:\")\n",
    "display(extracted_df[['url', 'mentioned_entities', 'keyword_matched_labels']].head(2))\n",
    "\n",
    "# Merge the keyword matched labels back to the main DataFrame\n",
    "# For demonstration, we'll only merge the processed sample\n",
    "# In production, you would process and merge the entire dataset\n",
    "df_final = pd.merge(df_final, extracted_df[['id', 'mentioned_entities', 'keyword_matched_labels']], \n",
    "                    on='id', how='left')\n",
    "\n",
    "# Fill NaN values with empty lists\n",
    "df_final['mentioned_entities'] = df_final['mentioned_entities'].apply(lambda x: [] if pd.isna(x) else x)\n",
    "df_final['keyword_matched_labels'] = df_final['keyword_matched_labels'].apply(lambda x: [] if pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Labels\n",
    "print(\"\\n=== Combining Labels from Different Strategies ===\\n\")\n",
    "\n",
    "# Ensure both label columns exist\n",
    "if 'ontology_labels_k' in df_final.columns and 'keyword_matched_labels' in df_final.columns:\n",
    "    # Create a unified list of labels from both strategies\n",
    "    def combine_labels(row):\n",
    "        # Get labels from both strategies\n",
    "        semantic_labels = row['ontology_labels_k'] if isinstance(row['ontology_labels_k'], list) else []\n",
    "        keyword_labels = row['keyword_matched_labels'] if isinstance(row['keyword_matched_labels'], list) else []\n",
    "        \n",
    "        # Combine and deduplicate\n",
    "        combined = list(set(semantic_labels + keyword_labels))\n",
    "        return combined\n",
    "    \n",
    "    # Apply the combination function\n",
    "    df_final['combined_ontology_labels'] = df_final.apply(combine_labels, axis=1)\n",
    "    \n",
    "    # Count labels\n",
    "    total_combined = df_final['combined_ontology_labels'].apply(len).sum()\n",
    "    total_semantic = df_final['ontology_labels_k'].apply(len).sum()\n",
    "    total_keyword = df_final['keyword_matched_labels'].apply(len).sum()\n",
    "    \n",
    "    print(f\"✅ Combined labels created\")\n",
    "    print(f\"Total semantic labels: {total_semantic}\")\n",
    "    print(f\"Total keyword labels: {total_keyword}\")\n",
    "    print(f\"Total combined unique labels: {total_combined}\")\n",
    "    \n",
    "    print(\"\\nSample comparison of label sources:\")\n",
    "    for i in range(2):\n",
    "        print(f\"URL: {df_final.iloc[i]['url']}\")\n",
    "        print(f\"Semantic Labels: {df_final.iloc[i]['ontology_labels_k']}\")\n",
    "        print(f\"Keyword Labels: {df_final.iloc[i]['keyword_matched_labels']}\")\n",
    "        print(f\"Combined Labels: {df_final.iloc[i]['combined_ontology_labels']}\")\n",
    "        print(\"---\")\n",
    "else:\n",
    "    print(\"⚠️ Warning: Required label columns not found. Skipping label combination.\")\n",
    "\n",
    "print(\"\\n✅ XMLC label assignment completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 7: Ontology Expansion Analysis\n",
    "\n",
    "In this section, we analyze the data to identify potential new ontology labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.1: Cluster Bookmarks by Content Similarity\n",
    "print(\"=== Clustering Bookmarks by Content Similarity ===\\n\")\n",
    "\n",
    "# Define the number of clusters\n",
    "n_clusters = 10  # Adjust based on dataset size and diversity\n",
    "\n",
    "# Use LOTUS sem_cluster operator to cluster the bookmarks\n",
    "clustered_df = lotus.sem_cluster(\n",
    "    df_final,\n",
    "    column=\"combined_text\",\n",
    "    n_clusters=n_clusters,\n",
    "    cluster_column_name=\"content_cluster\"\n",
    ")\n",
    "\n",
    "# Count bookmarks per cluster\n",
    "cluster_counts = clustered_df['content_cluster'].value_counts().sort_index()\n",
    "\n",
    "print(f\"✅ Clustered bookmarks into {n_clusters} groups\")\n",
    "print(\"\\nBookmarks per cluster:\")\n",
    "print(cluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.2: Extract Cluster Themes\n",
    "print(\"=== Extracting Cluster Themes ===\\n\")\n",
    "\n",
    "# Define the extraction prompt\n",
    "theme_prompt = \"\"\"\n",
    "Analyze the text and identify the main themes or topics.\n",
    "Focus on technical domains, concepts, and subject areas.\n",
    "\n",
    "Format your response as a JSON object with this structure:\n",
    "{\"cluster_theme\": \"Main theme\", \"subtopics\": [\"subtopic1\", \"subtopic2\", ...]}\n",
    "\"\"\"\n",
    "\n",
    "# Sample a few bookmarks from each cluster for theme extraction\n",
    "cluster_samples = []\n",
    "for cluster_id in range(n_clusters):\n",
    "    # Get bookmarks from this cluster\n",
    "    cluster_bookmarks = clustered_df[clustered_df['content_cluster'] == cluster_id]\n",
    "    \n",
    "    # Skip empty clusters\n",
    "    if len(cluster_bookmarks) == 0:\n",
    "        continue\n",
    "        \n",
    "    # Sample up to 5 bookmarks from this cluster\n",
    "    sample_size = min(5, len(cluster_bookmarks))\n",
    "    samples = cluster_bookmarks.sample(sample_size)\n",
    "    \n",
    "    # Concatenate the text from all samples\n",
    "    combined_sample_text = \" \".join(samples['combined_text'].tolist())\n",
    "    \n",
    "    # Add to the list of samples\n",
    "    cluster_samples.append({\n",
    "        'cluster_id': cluster_id,\n",
    "        'sample_text': combined_sample_text,\n",
    "        'bookmark_count': len(cluster_bookmarks)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "cluster_samples_df = pd.DataFrame(cluster_samples)\n",
    "\n",
    "# Extract themes using LOTUS sem_extract\n",
    "if len(cluster_samples_df) > 0:\n",
    "    themes_df = lotus.sem_extract(\n",
    "        cluster_samples_df,\n",
    "        column=\"sample_text\",\n",
    "        output_format={\"cluster_theme\": \"Main theme\", \"subtopics\": \"List of subtopics\"},\n",
    "        prompt=theme_prompt\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Extracted themes for {len(themes_df)} clusters\")\n",
    "    print(\"\\nSample of cluster themes:\")\n",
    "    for i in range(min(3, len(themes_df))):\n",
    "        print(f\"Cluster {themes_df.iloc[i]['cluster_id']} ({themes_df.iloc[i]['bookmark_count']} bookmarks):\")\n",
    "        print(f\"  Theme: {themes_df.iloc[i]['cluster_theme']}\")\n",
    "        print(f\"  Subtopics: {themes_df.iloc[i]['subtopics']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"⚠️ No clusters available for theme extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.3: Identify Potential New Ontology Labels\n",
    "print(\"=== Identifying Potential New Ontology Labels ===\\n\")\n",
    "\n",
    "# Collect all existing ontology labels\n",
    "existing_labels = set()\n",
    "for labels in df_final['combined_ontology_labels']:\n",
    "    if isinstance(labels, list):\n",
    "        existing_labels.update(labels)\n",
    "\n",
    "# Extract potential new labels from cluster themes\n",
    "if 'themes_df' in locals() and len(themes_df) > 0:\n",
    "    # Collect all themes and subtopics\n",
    "    potential_labels = set()\n",
    "    for _, row in themes_df.iterrows():\n",
    "        # Add the main theme\n",
    "        if isinstance(row['cluster_theme'], str):\n",
    "            potential_labels.add(row['cluster_theme'])\n",
    "        \n",
    "        # Add all subtopics\n",
    "        if isinstance(row['subtopics'], list):\n",
    "            potential_labels.update(row['subtopics'])\n",
    "    \n",
    "    # Find labels that don't exist in the current ontology\n",
    "    new_labels = potential_labels - existing_labels\n",
    "    \n",
    "    print(f\"✅ Identified {len(new_labels)} potential new ontology labels\")\n",
    "    print(\"\\nSample of potential new labels:\")\n",
    "    print(list(new_labels)[:10])  # Show up to 10 new labels\n",
    "else:\n",
    "    print(\"⚠️ No cluster themes available for new label identification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.4: Entity Extraction for Ontology Expansion\n",
    "print(\"=== Entity Extraction for Ontology Expansion ===\\n\")\n",
    "\n",
    "# Define the entity extraction prompt\n",
    "entity_prompt = \"\"\"\n",
    "Extract specific named entities from the text that could be valuable additions to a technical ontology.\n",
    "Focus on:\n",
    "1. Technical tools and frameworks\n",
    "2. Programming languages and libraries\n",
    "3. AI/ML models and techniques\n",
    "4. Technical concepts and methodologies\n",
    "\n",
    "Format your response as a JSON object with this structure:\n",
    "{\"extracted_entities\": [\"entity1\", \"entity2\", ...], \"entity_categories\": {\"category1\": [\"entity1\", \"entity2\"], \"category2\": [\"entity3\"]}}\n",
    "\"\"\"\n",
    "\n",
    "# Sample bookmarks for entity extraction\n",
    "sample_size = min(20, len(df_final))  # Process up to 20 rows for demonstration\n",
    "entity_sample_df = df_final.sample(sample_size).copy()\n",
    "\n",
    "# Extract entities using LOTUS sem_extract\n",
    "entities_df = lotus.sem_extract(\n",
    "    entity_sample_df,\n",
    "    column=\"combined_text\",\n",
    "    output_format={\n",
    "        \"extracted_entities\": \"List of entities\",\n",
    "        \"entity_categories\": \"Categorized entities\"\n",
    "    },\n",
    "    prompt=entity_prompt\n",
    ")\n",
    "\n",
    "print(f\"✅ Extracted entities from {len(entities_df)} bookmarks\")\n",
    "print(\"\\nSample of extracted entities:\")\n",
    "for i in range(min(2, len(entities_df))):\n",
    "    print(f\"Bookmark: {entities_df.iloc[i]['url']}\")\n",
    "    print(f\"  Entities: {entities_df.iloc[i]['extracted_entities'][:5]}...\" if len(entities_df.iloc[i]['extracted_entities']) > 5 else f\"  Entities: {entities_df.iloc[i]['extracted_entities']}\")\n",
    "    print(f\"  Categories: {entities_df.iloc[i]['entity_categories']}\")\n",
    "    print()\n",
    "\n",
    "# Aggregate all extracted entities\n",
    "all_entities = []\n",
    "for entities in entities_df['extracted_entities']:\n",
    "    if isinstance(entities, list):\n",
    "        all_entities.extend(entities)\n",
    "\n",
    "# Count entity frequencies\n",
    "from collections import Counter\n",
    "entity_counts = Counter(all_entities)\n",
    "top_entities = entity_counts.most_common(20)  # Get top 20 entities\n",
    "\n",
    "print(\"\\nTop extracted entities:\")\n",
    "for entity, count in top_entities:\n",
    "    print(f\"  {entity}: {count} occurrences\")\n",
    "\n",
    "print(\"\\n✅ Ontology expansion analysis completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 8: Store Enriched Data\n",
    "\n",
    "In this section, we save the enriched data to files for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.1: Prepare Output Directory\n",
    "print(\"=== Preparing Output Directory ===\\n\")\n",
    "\n",
    "# Create output directory with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"enriched_data_{timestamp}\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"✅ Created output directory: {output_dir}\")\n",
    "else:\n",
    "    print(f\"⚠️ Output directory already exists: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.2: Save Full Enriched Dataset\n",
    "print(\"=== Saving Full Enriched Dataset ===\\n\")\n",
    "\n",
    "# Save as CSV (for easy viewing and basic analysis)\n",
    "csv_path = os.path.join(output_dir, \"enriched_bookmarks.csv\")\n",
    "\n",
    "# Select columns to save\n",
    "# Exclude large or complex columns that don't work well in CSV\n",
    "csv_columns = [col for col in df_final.columns if not any(x in col for x in ['metadata', '_scores', '_paths'])]\n",
    "\n",
    "# For list columns, convert to string representation\n",
    "df_csv = df_final[csv_columns].copy()\n",
    "for col in df_csv.columns:\n",
    "    if df_csv[col].apply(lambda x: isinstance(x, list)).any():\n",
    "        df_csv[col] = df_csv[col].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Save to CSV\n",
    "df_csv.to_csv(csv_path, index=False)\n",
    "print(f\"✅ Saved CSV file: {csv_path}\")\n",
    "\n",
    "# Save as JSONL (preserving all data structures)\n",
    "jsonl_path = os.path.join(output_dir, \"enriched_bookmarks.jsonl\")\n",
    "\n",
    "# Convert DataFrame to JSONL\n",
    "with open(jsonl_path, 'w') as f:\n",
    "    for _, row in df_final.iterrows():\n",
    "        # Convert row to dictionary and handle non-serializable objects\n",
    "        row_dict = row.to_dict()\n",
    "        for key, value in row_dict.items():\n",
    "            if pd.isna(value):\n",
    "                row_dict[key] = None\n",
    "            elif isinstance(value, pd.Timestamp):\n",
    "                row_dict[key] = value.isoformat()\n",
    "        \n",
    "        # Write as JSON line\n",
    "        f.write(json.dumps(row_dict) + '\\n')\n",
    "\n",
    "print(f\"✅ Saved JSONL file: {jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.3: Save Label Statistics\n",
    "print(\"=== Saving Label Statistics ===\\n\")\n",
    "\n",
    "# Create a DataFrame with label statistics\n",
    "label_stats = []\n",
    "\n",
    "# Check if combined_ontology_labels column exists\n",
    "if 'combined_ontology_labels' in df_final.columns:\n",
    "    # Collect all labels\n",
    "    all_labels = []\n",
    "    for labels in df_final['combined_ontology_labels']:\n",
    "        if isinstance(labels, list):\n",
    "            all_labels.extend(labels)\n",
    "    \n",
    "    # Count label frequencies\n",
    "    from collections import Counter\n",
    "    label_counts = Counter(all_labels)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    for label, count in label_counts.most_common():\n",
    "        label_stats.append({\n",
    "            'label': label,\n",
    "            'count': count,\n",
    "            'percentage': round(count / len(df_final) * 100, 2)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    label_stats_df = pd.DataFrame(label_stats)\n",
    "    \n",
    "    # Save to CSV\n",
    "    stats_path = os.path.join(output_dir, \"label_statistics.csv\")\n",
    "    label_stats_df.to_csv(stats_path, index=False)\n",
    "    \n",
    "    print(f\"✅ Saved label statistics: {stats_path}\")\n",
    "    print(f\"Total unique labels: {len(label_stats_df)}\")\n",
    "    print(\"\\nTop 10 labels:\")\n",
    "    for i in range(min(10, len(label_stats_df))):\n",
    "        print(f\"  {label_stats_df.iloc[i]['label']}: {label_stats_df.iloc[i]['count']} bookmarks ({label_stats_df.iloc[i]['percentage']}%)\")\n",
    "else:\n",
    "    print(\"⚠️ No 'combined_ontology_labels' column found. Skipping label statistics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.4: Save Summary Report\n",
    "print(\"=== Saving Summary Report ===\\n\")\n",
    "\n",
    "# Create a summary report\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'total_bookmarks': len(df_final),\n",
    "    'output_directory': output_dir,\n",
    "    'files_created': [\n",
    "        {'name': 'enriched_bookmarks.csv', 'path': csv_path},\n",
    "        {'name': 'enriched_bookmarks.jsonl', 'path': jsonl_path}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add label statistics if available\n",
    "if 'label_stats_df' in locals():\n",
    "    summary['total_unique_labels'] = len(label_stats_df)\n",
    "    summary['files_created'].append({'name': 'label_statistics.csv', 'path': stats_path})\n",
    "\n",
    "# Save summary as JSON\n",
    "summary_path = os.path.join(output_dir, \"summary.json\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"✅ Saved summary report: {summary_path}\")\n",
    "\n",
    "print(\"\\n✅ Data storage completed successfully\")\n",
    "print(f\"All enriched data saved to directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 9: Finish Task & Summarize\n",
    "\n",
    "In this section, we summarize the execution and provide final insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution Summary\n",
    "print(\"=== XMLC-LOTUS Execution Summary ===\\n\")\n",
    "\n",
    "# Collect key statistics\n",
    "stats = {\n",
    "    'total_bookmarks_processed': len(df_final),\n",
    "    'total_ontology_labels': len(all_label_names) if 'all_label_names' in locals() else 0,\n",
    "    'total_labels_assigned': df_final['combined_ontology_labels'].apply(len).sum() if 'combined_ontology_labels' in df_final.columns else 0,\n",
    "    'avg_labels_per_bookmark': round(df_final['combined_ontology_labels'].apply(len).mean(), 2) if 'combined_ontology_labels' in df_final.columns else 0,\n",
    "    'output_directory': output_dir if 'output_dir' in locals() else 'Not saved',\n",
    "    'execution_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Print summary\n",
    "print(f\"Bookmarks Processed: {stats['total_bookmarks_processed']}\")\n",
    "print(f\"Ontology Labels Available: {stats['total_ontology_labels']}\")\n",
    "print(f\"Total Labels Assigned: {stats['total_labels_assigned']}\")\n",
    "print(f\"Average Labels per Bookmark: {stats['avg_labels_per_bookmark']}\")\n",
    "print(f\"Output Directory: {stats['output_directory']}\")\n",
    "print(f\"Execution Completed: {stats['execution_timestamp']}\")\n",
    "\n",
    "print(\"\\n✅ XMLC-LOTUS pipeline execution completed successfully\")\n",
    "print(\"The bookmark data has been enriched with ontology labels and is ready for further use.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
